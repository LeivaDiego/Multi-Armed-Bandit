{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Laboratorio #2 \n",
        "## Multi-armed Bandit \n",
        "### CC3104 Reinforcement Learning "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ym5VtTlSNavN",
        "outputId": "9c7e1340-da40-414c-9472-347cb84c20b8"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        " multiarmed_bandit.py  (author: Anson Wong / git: ankonzoid)\n",
        "\n",
        " We solve the multi-armed bandit problem using a classical epsilon-greedy\n",
        " agent with reward-average sampling as the estimate to action-value Q.\n",
        " This algorithm follows closely with the notation of Sutton's RL textbook.\n",
        "\n",
        " We set up bandit arms with fixed probability distribution of success,\n",
        " and receive stochastic rewards from each arm of +1 for success,\n",
        " and 0 reward for failure.\n",
        "\n",
        " The incremental update rule action-value Q for each (action a, reward r):\n",
        "   n += 1\n",
        "   Q(a) <- Q(a) + 1/n * (r - Q(a))\n",
        "\n",
        " where:\n",
        "   n = number of times action \"a\" was performed\n",
        "   Q(a) = value estimate of action \"a\"\n",
        "   r(a) = reward of sampling action bandit (bandit) \"a\"\n",
        "\n",
        " Derivation of the Q incremental update rule:\n",
        "   Q_{n+1}(a)\n",
        "   = 1/n * (r_1(a) + r_2(a) + ... + r_n(a))\n",
        "   = 1/n * ((n-1) * Q_n(a) + r_n(a))\n",
        "   = 1/n * (n * Q_n(a) + r_n(a) - Q_n(a))\n",
        "   = Q_n(a) + 1/n * (r_n(a) - Q_n(a))\n",
        "\n",
        "\"\"\"\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "np.random.seed(0)\n",
        "\n",
        "class Environment:\n",
        "    def __init__(self, probs):\n",
        "       # Success probabilities for each arm.\n",
        "        self.probs = probs\n",
        "\n",
        "    def step(self, action):\n",
        "        # Pull arm and get stochastic reward (1 for success, 0 for failure)\n",
        "        return 1 if (np.random.random()  < self.probs[action]) else 0\n",
        "\n",
        "class Agent:\n",
        "\n",
        "    def __init__(self, nActions, eps, strategy, min_eps, decay_rate):\n",
        "        self.nActions = nActions # Number of actions\n",
        "        self.eps = eps # probability of exploration vs exploitation.\n",
        "        self.n = np.zeros(nActions, dtype=np.int) # action counts n(a)\n",
        "        self.Q = np.zeros(nActions, dtype=np.float) # value Q(a)\n",
        "        \n",
        "        self.strategy = strategy\n",
        "        self.min_eps = min_eps\n",
        "        self.decay_rate = decay_rate\n",
        "        self.time_step = 0\n",
        "\n",
        "\n",
        "    def update_Q(self, action, reward):\n",
        "        # Update Q action-value given (action, reward)\n",
        "        self.n[action] += 1\n",
        "        self.Q[action] += (1.0/self.n[action]) * (reward - self.Q[action])\n",
        "\n",
        "    def explore_exploit(self, epsilon):\n",
        "        \"\"\"\n",
        "        Explore or exploit based on epsilon value.\n",
        "        Args:\n",
        "            epsilon (float): Probability of exploration (0 <= epsilon <= 1)\n",
        "        Returns:\n",
        "            int: Action index corresponding to the arm taken.\n",
        "        \"\"\"\n",
        "        # Epsilon-greedy exploration-exploitation\n",
        "        if np.random.random() < epsilon:\n",
        "            return np.random.randint(self.nActions) # Randomly explore\n",
        "        else:\n",
        "            return np.argmax(self.Q) # Exploit the best action\n",
        "\n",
        "    def get_action(self):\n",
        "        \"\"\" \n",
        "        Get action based on the current strategy. \n",
        "        1. Greedy: Always exploit the best action.\n",
        "        2. Static Epsilon-Greedy: Explore or exploit based on a static epsilon.\n",
        "        3. Decaying Epsilon-Greedy: Explore or exploit based on a decaying epsilon.\n",
        "\n",
        "        Args:\n",
        "            None\n",
        "\n",
        "        Returns:\n",
        "            int: Action index corresponding to the arm taken.\n",
        "        \"\"\"\n",
        "        # Greedy strategy\n",
        "        if self.strategy == 'greedy':\n",
        "            return np.argmax(self.Q) # Always exploit the best action\n",
        "        \n",
        "        # Epsilon-greedy with static epsilon strategy\n",
        "        elif self.strategy == 'static_eps':\n",
        "            return self.explore_exploit(self.eps) # Explore or exploit based on static epsilon\n",
        "            \n",
        "        # Epsilon-greedy with decaying epsilon strategy\n",
        "        elif self.strategy == 'decaying_eps':\n",
        "            # decay epsilon over time exponentially\n",
        "            current_epsilon = max(self.min_eps, self.eps * np.exp(-self.decay_rate * self.time_step))\n",
        "            self.time_step += 1 # increment time step\n",
        "            return self.explore_exploit(current_epsilon) # Explore or exploit based on decaying epsilon\n",
        "        \n",
        "        # If an unknown strategy is provided, raise an error\n",
        "        else:\n",
        "            raise ValueError(\"Unknown strategy: {}\".format(self.strategy))\n",
        "\n",
        "\n",
        "def experiment(probs, N_episodes, eps, strategy, min_eps, decay_rate):\n",
        "    \"\"\"Start multi-armed bandit simulation\"\"\"\n",
        "    # Initialize environment and agent\n",
        "    env = Environment(probs) # initialize arm probabilities\n",
        "    agent = Agent(len(env.probs), eps, strategy, min_eps, decay_rate)  # initialize agent\n",
        "\n",
        "    # Get the best action based on arm probabilities \n",
        "    best_action = np.argmax(env.probs)  # best action based on arm probabilities\n",
        "    actions, rewards, best_hits = [], [] # lists to store actions, rewards and best hits\n",
        "\n",
        "    # Run the experiment for N_episodes\n",
        "    for _ in range(N_episodes):\n",
        "        action = agent.get_action() # sample policy # integer correcponding to the arm taken.\n",
        "        reward = env.step(action) # take step + get reward # integer 0/1\n",
        "        agent.update_Q(action, reward) # update Q\n",
        "        \n",
        "        \n",
        "        actions.append(action) # list of ints\n",
        "        rewards.append(reward) # list of ints\n",
        "        best_hits.append(1 if action == best_action else 0) # 1 if action is the best action, else 0\n",
        "\n",
        "    return np.array(actions), np.array(rewards), np.array(best_hits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Settings\n",
        "probs = [0.10, 0.50, 0.60, 0.80, 0.10, 0.25, 0.60, 0.45, 0.75, 0.65] \n",
        "N_steps = 1000 # number of steps \n",
        "N_experiments = 5000 # number of experiments to perform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_experiment_batch(strategy, eps, min_eps, decay_rate, N_experiments, N_steps, probs):\n",
        "    \"\"\"\n",
        "    Run a batch of experiments for the multi-armed bandit problem. \n",
        "    Args:\n",
        "        strategy (str): Strategy to use ('greedy', 'static_eps', 'decaying_eps').\n",
        "        eps (float): Epsilon value for exploration (0 <= eps <= 1).\n",
        "        min_eps (float): Minimum epsilon value for decaying epsilon strategy.\n",
        "        decay_rate (float): Decay rate for the decaying epsilon strategy.\n",
        "        N_experiments (int): Number of experiments to run.\n",
        "    \n",
        "    Returns:\n",
        "        dict: A dictionary containing average reward, average best action,\n",
        "    \"\"\"\n",
        "    R = np.zeros(N_steps)   # Reward history sum\n",
        "    A = np.zeros((N_steps, len(probs))) # action history sum\n",
        "    best_hits_total = np.zeros(N_steps) # Total best hits across all experiments\n",
        "    \n",
        "    # Initialize progress bar to track experiments  \n",
        "    pbar = tqdm(range(N_experiments), desc=f\"{strategy} (ε={eps})\")\n",
        "\n",
        "    # Run experiments\n",
        "    for i in pbar:\n",
        "        # Run a single experiment with the given parameters\n",
        "        actions, rewards, best_hits = experiment(probs, N_steps, eps, strategy, min_eps, decay_rate)\n",
        "        \n",
        "        R += rewards # # Adding rewards for every time step across all experiments\n",
        "        best_hits_total += best_hits\n",
        "\n",
        "        for j, a in enumerate(actions):\n",
        "            # Each cell holds number of actions a in time step j across all experiments\n",
        "            A[j][a] += 1 # increment action count for action a at time step j\n",
        "\n",
        "        # Calculate average reward for the current experiment\n",
        "        reward_avg = np.sum(rewards) / len(rewards)\n",
        "\n",
        "        # Update progress bar with current experiment status\n",
        "        pbar.set_postfix({\n",
        "            \"Experiment\": f\"{i+1}/{N_experiments}\",\n",
        "            \"Steps\": N_steps,\n",
        "            \"Reward Avg\": f\"{reward_avg:.3f}\"\n",
        "        })\n",
        "\n",
        "    # Calculate average metrics across all experiments\n",
        "    # Average reward, average best action, and cumulative reward\n",
        "    avg_reward = R / N_experiments\n",
        "    avg_best_action = best_hits_total / N_experiments\n",
        "    cumulative_reward = np.cumsum(R) / N_experiments\n",
        "\n",
        "    # Return the results as a dictionary\n",
        "    return {\n",
        "        \"avg_reward\": avg_reward,\n",
        "        \"avg_best_action\": avg_best_action,\n",
        "        \"cumulative_reward\": cumulative_reward,\n",
        "        \"action_counts\": A\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define strategies and epsilon values\n",
        "# Each strategy is a tuple of (strategy_name, epsilon_value)\n",
        "strategies = [\n",
        "    (\"greedy\", 0.0),\n",
        "    (\"static_eps\", 0.01),\n",
        "    (\"static_eps\", 0.05),\n",
        "    (\"static_eps\", 0.1),\n",
        "    (\"decaying_eps\", 0.01),\n",
        "    (\"decaying_eps\", 0.05),\n",
        "    (\"decaying_eps\", 0.1),\n",
        "]\n",
        "\n",
        "# Initialize results dictionary to store results for each strategy and epsilon value\n",
        "results = {}\n",
        "\n",
        "# Run simulations for each strategy and epsilon value\n",
        "# This will run the experiments in parallel for each strategy and epsilon value\n",
        "for strategy, eps_val in strategies:\n",
        "    # Print the current strategy and epsilon value being run\n",
        "    print(f\"Running strategy={strategy}, ε={eps_val}\")\n",
        "\n",
        "    # Run the simulation batch for the current strategy and epsilon value\n",
        "    # Store the results in the results dictionary with a tuple key (strategy, eps_val)\n",
        "    results[(strategy, eps_val)] = run_experiment_batch(\n",
        "        strategy=strategy,\n",
        "        eps=eps_val,\n",
        "        min_eps=0.01,\n",
        "        decay_rate=0.001,\n",
        "        N_experiments=N_experiments,\n",
        "        N_steps=N_steps,\n",
        "        probs=probs\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot reward results\n",
        "\n",
        "# 500 x 1 - each cell is average reward per timestep across all experiments.\n",
        "# Ideally this increases on average over time as the agent learns. Check rewards.png.\n",
        "R_avg =  R / np.float(N_experiments)\n",
        "plt.plot(R_avg, \".\")\n",
        "plt.xlabel(\"Step\")\n",
        "plt.ylabel(\"Average Reward\")\n",
        "plt.grid()\n",
        "ax = plt.gca()\n",
        "plt.xlim([1, N_steps])\n",
        "if save_fig:\n",
        "    if not os.path.exists(output_dir): os.mkdir(output_dir)\n",
        "    plt.savefig(os.path.join(output_dir, \"rewards.png\"), bbox_inches=\"tight\")\n",
        "else:\n",
        "    plt.show()\n",
        "plt.close()\n",
        "\n",
        "# Plot action results\n",
        "for i in range(len(probs)):\n",
        "    A_pct = 100 * A[:,i] / N_experiments # num_steps x 1 = 500 x 1\n",
        "    # Each cell is number of times the action i was selected for time step j across all experiments /  number of experiments conducted\n",
        "    steps = list(np.array(range(len(A_pct)))+1) # [0, 1, 2, 3 , 4, ..., 500]\n",
        "    # Plotting line chart for just 1 action i at a time.\n",
        "    plt.plot(steps, A_pct, \"-\",\n",
        "             linewidth=4,\n",
        "             label=\"Arm {} ({:.0f}%)\".format(i+1, 100*probs[i])) # Incrementing Arm + 1 as they start with 0 index\n",
        "    # We should ideally see as timesteps go on, the slot with the largest probability of success is chosen the most.\n",
        "    # check actions.png\n",
        "plt.xlabel(\"Step\")\n",
        "plt.ylabel(\"Count Percentage (%)\")\n",
        "leg = plt.legend(loc='upper left', shadow=True)\n",
        "plt.xlim([1, N_steps])\n",
        "plt.ylim([0, 100])\n",
        "for legobj in leg.legendHandles:\n",
        "    legobj.set_linewidth(4.0)\n",
        "if save_fig:\n",
        "    if not os.path.exists(output_dir): os.mkdir(output_dir)\n",
        "    plt.savefig(os.path.join(output_dir, \"actions.png\"), bbox_inches=\"tight\")\n",
        "else:\n",
        "    plt.show()\n",
        "plt.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "RL_lab2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
